{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute in the Terminal:\n",
    "\n",
    "Only once you have setup s3fs, you can execute the following command in Terminal:\n",
    "\n",
    "`s3fs your-name -o use_cache=/tmp -o allow_other -o uid=1001 -o mp_umask=002 -o multireq_max=5 ./DATA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from baseline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, z_dim=10, nc=3):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),                        # B, 256\n",
    "            nn.Linear(6400, z_dim*2),            # B, z_dim*2\n",
    "        )\n",
    "\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class BaseDecoder(nn.Module):\n",
    "    def __init__(self, z_dim=10, nc=3):\n",
    "        super(BaseDecoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 6400),               # B, 256\n",
    "            View((-1, 256, 5, 5)),               # B, 256,  1,  1\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n",
    "        )\n",
    "\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    def __init__(self, z_dim=10, nc=3):\n",
    "        super(BaseVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.encoder = BaseEncoder(z_dim, nc)\n",
    "        self.decoder = BaseDecoder(z_dim, nc)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self._encode(x)\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentEncoder(nn.Module):\n",
    "    def __init__(self, z_dim=10, nc=3):\n",
    "        super(StudentEncoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, 32, 4, 2, 1),  # B,  32, 32, 32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32, 16, 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # B,  64,  8,  8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1),  # B,  64,  4,  4\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 256, 4, 1),  # B, 256,  1,  1\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),  # B, 256\n",
    "            nn.Linear(6400, 3200),  # B, z_dim*2\n",
    "            nn.Linear(3200, z_dim * 2),  # B, z_dim*2\n",
    "        )\n",
    "\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class StudentDecoder(nn.Module):\n",
    "    def __init__(self, z_dim=20, nc=3):\n",
    "        super(StudentDecoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 3200),  # B, 256\n",
    "            nn.Linear(3200, 6400),  # B, 256\n",
    "            View((-1, 256, 5, 5)),  # B, 256,  1,  1\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 64, 4),  # B,  64,  4,  4\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1),  # B,  64,  8,  8\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # B,  32, 16, 16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, 4, 2, 1),  # B,  32, 32, 32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n",
    "        )\n",
    "\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "class StudentVAE(nn.Module):\n",
    "    def __init__(self, z_dim=20, nc=3):\n",
    "        super(StudentVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.encoder = StudentEncoder(z_dim, nc)\n",
    "        self.decoder = StudentDecoder(z_dim, nc)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self._encode(x)\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 3, 128, 128)\n",
    "\n",
    "x = BaseEncoder()(x)\n",
    "print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,568\n",
      "              ReLU-2           [-1, 32, 64, 64]               0\n",
      "            Conv2d-3           [-1, 32, 32, 32]          16,416\n",
      "              ReLU-4           [-1, 32, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          32,832\n",
      "              ReLU-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          65,600\n",
      "              ReLU-8             [-1, 64, 8, 8]               0\n",
      "            Conv2d-9            [-1, 256, 5, 5]         262,400\n",
      "             ReLU-10            [-1, 256, 5, 5]               0\n",
      "          Flatten-11                 [-1, 6400]               0\n",
      "           Linear-12                   [-1, 20]         128,020\n",
      "      BaseEncoder-13       [[-1, 10], [-1, 10]]               0\n",
      "           Linear-14                 [-1, 6400]          70,400\n",
      "             View-15            [-1, 256, 5, 5]               0\n",
      "             ReLU-16            [-1, 256, 5, 5]               0\n",
      "  ConvTranspose2d-17             [-1, 64, 8, 8]         262,208\n",
      "             ReLU-18             [-1, 64, 8, 8]               0\n",
      "  ConvTranspose2d-19           [-1, 64, 16, 16]          65,600\n",
      "             ReLU-20           [-1, 64, 16, 16]               0\n",
      "  ConvTranspose2d-21           [-1, 32, 32, 32]          32,800\n",
      "             ReLU-22           [-1, 32, 32, 32]               0\n",
      "  ConvTranspose2d-23           [-1, 32, 64, 64]          16,416\n",
      "             ReLU-24           [-1, 32, 64, 64]               0\n",
      "  ConvTranspose2d-25          [-1, 3, 128, 128]           1,539\n",
      "      BaseDecoder-26          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 955,799\n",
      "Trainable params: 955,799\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 6.67\n",
      "Params size (MB): 3.65\n",
      "Estimated Total Size (MB): 10.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = BaseVAE().cuda()\n",
    "summary(model, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,568\n",
      "              ReLU-2           [-1, 32, 64, 64]               0\n",
      "            Conv2d-3           [-1, 32, 32, 32]          16,416\n",
      "              ReLU-4           [-1, 32, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          32,832\n",
      "              ReLU-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          65,600\n",
      "              ReLU-8             [-1, 64, 8, 8]               0\n",
      "            Conv2d-9            [-1, 256, 5, 5]         262,400\n",
      "             ReLU-10            [-1, 256, 5, 5]               0\n",
      "          Flatten-11                 [-1, 6400]               0\n",
      "           Linear-12                 [-1, 3200]      20,483,200\n",
      "           Linear-13                   [-1, 40]         128,040\n",
      "   StudentEncoder-14       [[-1, 20], [-1, 20]]               0\n",
      "           Linear-15                 [-1, 3200]          67,200\n",
      "           Linear-16                 [-1, 6400]      20,486,400\n",
      "             View-17            [-1, 256, 5, 5]               0\n",
      "             ReLU-18            [-1, 256, 5, 5]               0\n",
      "  ConvTranspose2d-19             [-1, 64, 8, 8]         262,208\n",
      "             ReLU-20             [-1, 64, 8, 8]               0\n",
      "  ConvTranspose2d-21           [-1, 64, 16, 16]          65,600\n",
      "             ReLU-22           [-1, 64, 16, 16]               0\n",
      "  ConvTranspose2d-23           [-1, 32, 32, 32]          32,800\n",
      "             ReLU-24           [-1, 32, 32, 32]               0\n",
      "  ConvTranspose2d-25           [-1, 32, 64, 64]          16,416\n",
      "             ReLU-26           [-1, 32, 64, 64]               0\n",
      "  ConvTranspose2d-27          [-1, 3, 128, 128]           1,539\n",
      "   StudentDecoder-28          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 41,922,219\n",
      "Trainable params: 41,922,219\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 6.71\n",
      "Params size (MB): 159.92\n",
      "Estimated Total Size (MB): 166.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = StudentVAE().cuda()\n",
    "summary(model, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(net, C_max, use_cuda, max_iter, global_iter, decoder_dist,\n",
    "          beta, C_stop_iter, objective, gamma, optim):\n",
    "    net.train()\n",
    "    \n",
    "    logs_base_dir = \"runs\"\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "    train_summary_writer = SummaryWriter()\n",
    "    \n",
    "    if use_cuda == True:\n",
    "        C_max = Variable(torch.FloatTensor([C_max])).cuda()\n",
    "    else:\n",
    "        C_max = Variable(torch.FloatTensor([C_max]))\n",
    "    out = False\n",
    "\n",
    "    pbar = tqdm(total=max_iter)\n",
    "    pbar.update(global_iter)\n",
    "    \n",
    "    BCE_list = []\n",
    "    KLD_list = []\n",
    "    TL_list = []\n",
    "    \n",
    "    test_BCE_list = []\n",
    "    test_KLD_list = []\n",
    "    test_TL_list = []\n",
    "    \n",
    "    while not out:\n",
    "        for x in data_loader:\n",
    "            x = x.cuda()\n",
    "            net = net.cuda()\n",
    "            net.train()\n",
    "            \n",
    "            global_iter += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if use_cuda == True:\n",
    "                x = Variable(x).cuda()\n",
    "            else:\n",
    "                x = Variable(x)\n",
    "            \n",
    "            x_recon, mu, logvar = net(x)\n",
    "            recon_loss = reconstruction_loss(x, x_recon, decoder_dist)\n",
    "            total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "            beta_vae_loss = recon_loss + beta*total_kld\n",
    "                \n",
    "            BCE = recon_loss\n",
    "            KLD = total_kld\n",
    "            total_loss = beta_vae_loss\n",
    "\n",
    "            BCE_list.append(BCE.item())\n",
    "            KLD_list.append(KLD.item())\n",
    "            TL_list.append(total_loss.item())\n",
    "\n",
    "            beta_vae_loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            \n",
    "            net.eval()\n",
    "            \n",
    "            test_KLD = 0\n",
    "            test_BCE = 0\n",
    "            test_TL = 0\n",
    "            \n",
    "            for y in test_loader:\n",
    "                y = Variable(y)\n",
    "                y = y.cuda()\n",
    "                \n",
    "                y_recon, mu, logvar = net(y)\n",
    "                recon_loss = reconstruction_loss(y, y_recon, decoder_dist)\n",
    "                total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "                beta_vae_loss = recon_loss + beta*total_kld\n",
    "                \n",
    "                test_KLD += total_kld.item()\n",
    "                test_BCE += recon_loss.item()\n",
    "                test_TL += beta_vae_loss.item()\n",
    "                \n",
    "                del y\n",
    "                del dim_wise_kld\n",
    "                del mean_kld\n",
    "                del total_kld\n",
    "                del recon_loss\n",
    "                del beta_vae_loss\n",
    "                \n",
    "            test_BCE_list.append(test_BCE/14196)\n",
    "            test_KLD_list.append(test_KLD/14196)\n",
    "            test_TL_list.append(test_TL/14196)\n",
    "            \n",
    "            if global_iter >= max_iter:\n",
    "                out = True\n",
    "                break\n",
    "                \n",
    "\n",
    "    pbar.write(\"[Training Finished]\")\n",
    "    pbar.close()\n",
    "    \n",
    "    return BCE_list, KLD_list, TL_list, test_BCE_list, test_KLD_list, test_TL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YourName()\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1024, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = YourName(train = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, num_workers=8, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !rm -rf runs\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --port=6006 --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN(BETA_PARAM, return_model = False):\n",
    "    \n",
    "    C_max = 25\n",
    "    max_iter = len(data_loader)\n",
    "    # max_iter = 100\n",
    "    global_iter = 0\n",
    "    decoder_dist = \"gaussian\"\n",
    "    beta = 0.5\n",
    "    C_stop_iter = 1e5\n",
    "    gamma = 1000\n",
    "    use_cuda = True\n",
    "    # use_cuda = False\n",
    "    objective=\"H\"\n",
    "\n",
    "    if use_cuda == False:\n",
    "        net1 = BaseVAE()\n",
    "    else:\n",
    "        net1 = BaseVAE().cuda()\n",
    "\n",
    "    optim1 = torch.optim.Adam(net1.parameters(),\n",
    "                             lr=1e-4,\n",
    "                             betas=(0.9, 0.999))\n",
    "\n",
    "    PATH = \"./base-model-\" + str(BETA_PARAM) + \".pt\"\n",
    "    print(\"Now training:\", PATH)\n",
    "    if not os.path.exists(PATH):\n",
    "        BASELINE_BCE, BASELINE_KLD, BASELINE_TL, BASELINE_TEST_BCE, BASELINE_TEST_KLD, BASELINE_TEST_TL =\\\n",
    "            train(net1,\n",
    "                  C_max,\n",
    "                  use_cuda,\n",
    "                  max_iter,\n",
    "                  global_iter,\n",
    "                  decoder_dist,\n",
    "                  BETA_PARAM,\n",
    "                  C_stop_iter, \n",
    "                  objective,\n",
    "                  gamma,\n",
    "                  optim1)\n",
    "\n",
    "        PATH = \"./base-model-\"+str(BETA_PARAM)+\".pt\"\n",
    "\n",
    "        torch.save({'model_state_dict': net1.state_dict(),\n",
    "                    'BASELINE_BCE': BASELINE_BCE,\n",
    "                    'BASELINE_KLD': BASELINE_KLD,\n",
    "                    'BASELINE_TL': BASELINE_TL,\n",
    "                    'BASELINE_TEST_BCE': BASELINE_TEST_BCE,\n",
    "                    'BASELINE_TEST_KLD': BASELINE_TEST_KLD,\n",
    "                    'BASELINE_TEST_TL': BASELINE_TEST_TL,\n",
    "                    'BETA_PARAM': BETA_PARAM}, PATH)\n",
    "    \n",
    "    if use_cuda == False:\n",
    "        net2 = StudentVAE()\n",
    "    \n",
    "    else:\n",
    "        net2 = StudentVAE().cuda()\n",
    "    \n",
    "    optim2 = torch.optim.Adam(net2.parameters(), \n",
    "                             lr=1e-4,\n",
    "                             betas=(0.9, 0.999))\n",
    "    \n",
    "    PATH = \"./student-model-\" + str(BETA_PARAM) + \".pt\"\n",
    "    print(\"Now training:\", PATH)\n",
    "    if not os.path.exists(PATH):\n",
    "        OUR_MODEL_BCE, OUR_MODEL_KLD, OUR_MODEL_TL, OUR_MODEL_TEST_BCE, OUR_MODEL_TEST_KLD, OUR_MODEL_TEST_TL =\\\n",
    "            train(net2, \n",
    "                  C_max, \n",
    "                  use_cuda, \n",
    "                  max_iter, \n",
    "                  global_iter, \n",
    "                  decoder_dist,\n",
    "                  BETA_PARAM, \n",
    "                  C_stop_iter, \n",
    "                  objective,\n",
    "                  gamma, \n",
    "                  optim2)\n",
    "\n",
    "        PATH = \"./student-model-\"+str(BETA_PARAM)+\".pt\"\n",
    "\n",
    "        torch.save({'model_state_dict': net2.state_dict(),\n",
    "                    'OUR_MODEL_BCE': OUR_MODEL_BCE,\n",
    "                    'OUR_MODEL_KLD': OUR_MODEL_KLD,\n",
    "                    'OUR_MODEL_TL': OUR_MODEL_TL,\n",
    "                    'OUR_MODEL_TEST_BCE': OUR_MODEL_TEST_BCE,\n",
    "                    'OUR_MODEL_TEST_KLD': OUR_MODEL_TEST_KLD,\n",
    "                    'OUR_MODEL_TEST_TL': OUR_MODEL_TEST_TL,\n",
    "                    'BETA_PARAM': BETA_PARAM}, PATH)\n",
    "    \n",
    "    if return_model == True:\n",
    "        return net1, net2\n",
    "    \n",
    "#     else:\n",
    "#         return BASELINE_BCE, BASELINE_KLD, BASELINE_TL, OUR_MODEL_BCE, OUR_MODEL_KLD, OUR_MODEL_TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training: ./base-model-0.25.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/93 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training: ./student-model-0.25.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 35/93 [3:11:12<5:16:31, 327.44s/it]"
     ]
    }
   ],
   "source": [
    "# BCE_baseline_list = []\n",
    "# KLD_baseline_list = []\n",
    "# TL_baseline_list = []\n",
    "\n",
    "# BCE_ours_list = []\n",
    "# KLD_ours_list = []\n",
    "# TL_ours_list = []\n",
    "\n",
    "beta_params = [2**2,2**1,2**0,2**-1,2**-2]\n",
    "# Here the seq is reverse!!!!\n",
    "\n",
    "for beta_param in beta_params:\n",
    "    TRAIN(beta_param)\n",
    "    \n",
    "#     BASELINE_BCE, BASELINE_KLD, BASELINE_TL, OUR_MODEL_BCE, OUR_MODEL_KLD, OUR_MODEL_TL = TRAIN(beta_param)\n",
    "    \n",
    "#     BCE_baseline_list.append(BASELINE_BCE)\n",
    "#     KLD_baseline_list.append(BASELINE_KLD)\n",
    "#     TL_baseline_list.append(BASELINE_TL)\n",
    "\n",
    "#     BCE_ours_list.append(OUR_MODEL_BCE)\n",
    "#     KLD_ours_list.append(OUR_MODEL_KLD)\n",
    "#     TL_ours_list.append(OUR_MODEL_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = [\"./base-model-0.25.pt\",\n",
    "         \"./base-model-0.5.pt\",\n",
    "         \"./base-model-1.pt\",\n",
    "         \"./base-model-2.pt\",\n",
    "         \"./base-model-4.pt\"]\n",
    "\n",
    "PATH2 = [\"./student-model-0.25.pt\",\n",
    "         \"./student-model-0.5.pt\",\n",
    "         \"./student-model-1.pt\",\n",
    "         \"./student-model-2.pt\",\n",
    "         \"./student-model-4.pt\"]\n",
    "\n",
    "BASELINE_BCE = []\n",
    "BASELINE_KLD = []\n",
    "BASELINE_TL = []\n",
    "\n",
    "BASELINE_TEST_BCE = []\n",
    "BASELINE_TEST_KLD = []\n",
    "BASELINE_TEST_TL = []\n",
    "\n",
    "OUR_MODEL_BCE = []\n",
    "OUR_MODEL_KLD = []\n",
    "OUR_MODEL_TL = []\n",
    "\n",
    "OUR_MODEL_TEST_BCE = []\n",
    "OUR_MODEL_TEST_KLD = []\n",
    "OUR_MODEL_TEST_TL = []\n",
    "\n",
    "for path1, path2 in zip(PATH1, PATH2):\n",
    "    checkpoint = torch.load(path1, map_location=torch.device('cpu'))\n",
    "    BASELINE_BCE.append(checkpoint['BASELINE_BCE'])\n",
    "    BASELINE_KLD.append(checkpoint['BASELINE_KLD'])\n",
    "    BASELINE_TL.append(checkpoint['BASELINE_TL'])\n",
    "    \n",
    "    BASELINE_TEST_BCE.append(checkpoint['BASELINE_TEST_BCE'])\n",
    "    BASELINE_TEST_KLD.append(checkpoint['BASELINE_TEST_KLD'])\n",
    "    BASELINE_TEST_TL.append(checkpoint['BASELINE_TEST_TL'])\n",
    "    \n",
    "    checkpoint = torch.load(path2, map_location=torch.device('cpu'))\n",
    "    OUR_MODEL_BCE.append(checkpoint['OUR_MODEL_BCE'])\n",
    "    OUR_MODEL_KLD.append(checkpoint['OUR_MODEL_KLD'])\n",
    "    OUR_MODEL_TL.append(checkpoint['OUR_MODEL_TL'])\n",
    "    \n",
    "    OUR_MODEL_TEST_BCE.append(checkpoint['OUR_MODEL_TEST_BCE'])\n",
    "    OUR_MODEL_TEST_KLD.append(checkpoint['OUR_MODEL_TEST_KLD'])\n",
    "    OUR_MODEL_TEST_TL.append(checkpoint['OUR_MODEL_TEST_TL'])\n",
    "    \n",
    "\n",
    "for path1, path2 in zip(PATH1, PATH2):\n",
    "    checkpoint = torch.load(path1, map_location=torch.device('cpu'))\n",
    "    \n",
    "    checkpoint = torch.load(path2, map_location=torch.device('cpu'))\n",
    "    \n",
    "    del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"BCE\"\n",
    "\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_BCE[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_BCE[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    if i >= 24:\n",
    "        grid[i].set_xlabel(\"Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"KLD\"\n",
    "\n",
    "# cols = [\"Beta = 1\"]\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_KLD[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_KLD[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    grid[i].set_xlabel(\"Batch Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"Total Loss\"\n",
    "\n",
    "# cols = [\"Beta = 1\"]\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_TL[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_TL[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    grid[i].set_xlabel(\"Batch Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"BCE\"\n",
    "\n",
    "# cols = [\"Beta = 1\"]\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_TEST_BCE[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_TEST_BCE[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    grid[i].set_xlabel(\"Batch Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"KLD\"\n",
    "\n",
    "# cols = [\"Beta = 1\"]\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_TEST_KLD[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_TEST_KLD[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    grid[i].set_xlabel(\"Batch Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (25., 5.))\n",
    "\n",
    "metric = \"Total Loss\"\n",
    "\n",
    "# cols = [\"Beta = 1\"]\n",
    "res = [\"Resolution = 128x128\"]\n",
    "\n",
    "cols = [\"Beta = 0.25\", \"Beta = 0.5\", \"Beta = 1\", \"Beta = 2\", \"Beta = 4\"]\n",
    "\n",
    "# res = [\"Resolution = 142p\", \n",
    "#        \"Resolution = 243p\", \n",
    "#        \"Resolution = 320p\", \n",
    "#        \"Resolution = 480p\", \n",
    "#        \"Resolution = 720p\", \n",
    "#        \"Resolution = 1080p\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(res), len(cols)),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "for i in range(len(cols)*len(res)):\n",
    "\n",
    "\n",
    "    data1 = np.array([d for d in BASELINE_TEST_TL[i]])\n",
    "    data2 = np.array([d for d in OUR_MODEL_TEST_TL[i]])\n",
    "    \n",
    "    iterations = range(len(data1))\n",
    "    \n",
    "    grid[i].plot(iterations, data1, 'g', label='Baseline')\n",
    "    grid[i].plot(iterations, data2, 'b', label='Our Model')\n",
    "    grid[i].legend(loc=\"upper left\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        grid[i].set_ylabel(metric, rotation=90, size='large')\n",
    "\n",
    "\n",
    "    if i % 5 == 4:\n",
    "        grid[i].annotate(res[i//5],xy=(1.1,0.5), rotation=270, size='large',\n",
    "                        ha='center',va='center',xycoords='axes fraction')\n",
    "\n",
    "    grid[i].set_xlabel(\"Batch Iteration\", rotation=0, size='large')\n",
    "\n",
    "for ax, col in zip(grid, cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section on Scene Identificiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_PARAM = 2\n",
    "base, student = TRAIN(BETA_PARAM, return_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseLatentSpace = BaseEncoder().cpu()\n",
    "BaseLatentSpace.load_state_dict(base.cpu().encoder.state_dict())\n",
    "\n",
    "del base\n",
    "\n",
    "StudentLatentSpace = StudentEncoder().cpu()\n",
    "StudentLatentSpace.load_state_dict(student.cpu().encoder.state_dict())\n",
    "\n",
    "del student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_Div(mu0, mu1, var0, var1):\n",
    "    KLD = 0.5 * (var0 / var1) + (mu1 - mu0) * (1 / var1) * (mu1 - mu0) - 1 + np.log(var1 / var0)\n",
    "\n",
    "    return KLD\n",
    "\n",
    "def KL_Div_Loss(mu0_vector, mu1_vector, var0_vector, var1_vector):\n",
    "    total_KLD = 0\n",
    "\n",
    "    for (mu0, mu1, var0, var1) in zip(mu0_vector, mu1_vector, var0_vector, var1_vector):\n",
    "        total_KLD += KL_Div(mu0, mu1, var0, var1)\n",
    "\n",
    "    return total_KLD\n",
    "\n",
    "def multi_KLD(LatentSpace):\n",
    "    mu = None\n",
    "    var = None\n",
    "    \n",
    "    max_idx = len(data_loader)\n",
    "    \n",
    "    KLD = []\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        \n",
    "        if idx == 0:\n",
    "            mu0, var0 = LatentSpace(batch)\n",
    "            mu0, var0 = mu0.detach().numpy().squeeze(0), var0.detach().numpy().squeeze(0)\n",
    "            \n",
    "            print(\"mu shape:\", mu0.shape)\n",
    "            print(\"mu shape:\", var0.shape)\n",
    "            \n",
    "        else:\n",
    "            mu1, var1 = LatentSpace(batch)\n",
    "            mu1, var1 = mu1.detach().numpy().squeeze(0), var1.detach().numpy().squeeze(0)\n",
    "            \n",
    "            KLD.append(KL_Div_Loss(mu0, mu1, var0, var1))\n",
    "            \n",
    "            mu0, var0 = mu1, var1\n",
    "            \n",
    "        if (idx+1) % 10000 == 0:\n",
    "            print(idx, \"/\", max_idx)\n",
    "            \n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseLatentSpace.eval()\n",
    "baseKLD = multi_KLD(BaseLatentSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentLatentSpace.eval()\n",
    "studentKLD = multi_KLD(StudentLatentSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([KLD if KLD < 1000 else 0 for KLD in baseKLD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(1, (20., 10.))\n",
    "\n",
    "# rows = [\"KL-Div (Our Model)\", \"KL-Div (Baseline)\", \"L1 Loss\", \"L2 Loss\"]\n",
    "# rows = [\"KL-Div (Our Model)\", \"KL-Div (Baseline)\"]\n",
    "rows = [\"KL-Div (Our Model)\"]\n",
    "\n",
    "grid = Grid(fig, \n",
    "            rect=111,\n",
    "            nrows_ncols=(len(rows), 1),\n",
    "            axes_pad=0.1)\n",
    "\n",
    "# data = [studentKLD, baseKLD]\n",
    "data = [[KLD if KLD < 1000 else 0 for KLD in baseKLD]]\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "#     epochs = range(len(data[i]))\n",
    "    iterations = range(0, 14195)\n",
    "    \n",
    "    TRUE_SCENES_LIST # length 259\n",
    "    PRED_SCENES_LIST # length 259\n",
    "    \n",
    "    \n",
    "    \n",
    "    grid[i].plot(iterations, [data[i][j] for j in iterations], 'g', label='Distance')\n",
    "\n",
    "    grid[i].set_ylabel(rows[i], rotation=90, size='large')\n",
    "    if i == 3:\n",
    "        grid[i].set_xlabel(\"Iteration\", rotation=0, size='large')\n",
    "        \n",
    "    \n",
    "#     # TODO: Check if top 259 scenes are correctly identified\n",
    "    \n",
    "#     assert(len(TRUE_SCENES_LIST) == 259)\n",
    "#     assert(len(PRED_SCENES_LIST) == 259)\n",
    "    \n",
    "#     correct = 0 \n",
    "#     for true_scene in TRUE_SCENES_LIST:\n",
    "        \n",
    "#         if true_scene in PRED_SCENES_LIST:\n",
    "#             plt.axvline(true_scene,  label='pyplot vertical line', color = \"b\")\n",
    "#             correct += 1\n",
    "#         else::\n",
    "#             plt.axvline(true_scene,  label='pyplot vertical line', color = \"r\")\n",
    "            \n",
    "    \n",
    "#     test_accuracy = correct/len(TRUE_SCENES_LIST)\n",
    "            \n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}